# Migration from AV-HuBERT to LipCoordNet

## Summary

Successfully pivoted from AV-HuBERT (complex Fairseq model) to **LipCoordNet** (HuggingFace Hub model) for much simpler deployment and maintenance.

## What Changed

### Removed
- âŒ `sagemaker/vendor/av_hubert/` - 100+ MB vendored code
- âŒ `sagemaker/checkpoints/` - 1.8GB model checkpoint files
- âŒ `*.tar.gz` model artifacts (4GB+)
- âŒ `sagemaker/inference.py` - Complex Fairseq inference handler
- âŒ Fairseq dependencies (fairseq, python_speech_features, etc.)

### Added
- âœ… `sagemaker/inference_lipcoordnet.py` - Clean HuggingFace inference handler
- âœ… `sagemaker/requirements_lipcoordnet.txt` - Simplified dependencies
- âœ… `scripts/deploy_lipcoordnet.py` - HuggingFace deployment script

## Key Improvements

1. **No More Vendoring**: Model loads directly from HuggingFace Hub
2. **Simpler Dependencies**: transformers + torch + opencv instead of fairseq ecosystem
3. **Better Performance**: LipCoordNet has 0.6% CER vs 6.7% for base AV-HuBERT
4. **Easier Deployment**: Uses HuggingFaceModel class, no custom tar.gz building
5. **GPU Support**: Works on ml.g5.xlarge instances

## Model Details

**LipCoordNet** (SilentSpeak/LipCoordNet)
- State-of-the-art lip reading model
- Dual input: video frames (128x64) + lip landmark coordinates
- Performance: 0.6% CER, 1.7% WER on validation set
- Based on LipNet with enhanced landmark tracking

## Architecture Changes

### Input Format
```json
{
  "s3_bucket": "bucket-name",
  "s3_video_key": "path/to/video.mp4",
  "fps": 25
}
```

### Processing Pipeline
1. Download video from S3
2. Extract frames at target FPS
3. Detect face and extract lip landmarks (dlib)
4. Crop mouth ROI and resize to 128x64
5. Feed frames + landmarks to model
6. Decode CTC output to text

### Output Format
```json
{
  "text": "transcribed text",
  "status": "success",
  "num_frames": 150
}
```

## Deployment

### Quick Start
```bash
python scripts/deploy_lipcoordnet.py \
  --endpoint-name gorggle-lipcoordnet-dev \
  --role-arn arn:aws:iam::ACCOUNT:role/SageMakerRole \
  --instance-type ml.g5.xlarge \
  --region us-east-1
```

### Update Existing Endpoint
```bash
python scripts/deploy_lipcoordnet.py \
  --endpoint-name gorggle-lipcoordnet-dev \
  --role-arn arn:aws:iam::ACCOUNT:role/SageMakerRole \
  --instance-type ml.g5.xlarge \
  --region us-east-1 \
  --update
```

## Next Steps

1. **Deploy** - Run deployment script to create endpoint
2. **Test** - Invoke with sample video
3. **Update Lambda** - Modify `lambdas/invoke_lipreading/handler.py` to use new endpoint
4. **Monitor** - Check CloudWatch logs and metrics

## Dependencies

### Required Python Packages
- `transformers>=4.35.0` - HuggingFace model loading
- `torch>=2.0.0` - PyTorch framework
- `opencv-python-headless==4.8.1.78` - Video processing
- `numpy>=1.24.0` - Array operations
- `dlib>=19.24.0` - Face and landmark detection
- `boto3>=1.28.0` - AWS S3 access

### Optional Enhancement
- Download `shape_predictor_68_face_landmarks.dat` from dlib for better landmark detection
- Place in model artifact or `/opt/ml/model/` directory

## Testing

### Test Payload
```json
{
  "s3_bucket": "gorggle-dev-uploads",
  "s3_video_key": "test-video.mp4",
  "fps": 25
}
```

### AWS CLI Test
```bash
aws sagemaker-runtime invoke-endpoint \
  --endpoint-name gorggle-lipcoordnet-dev \
  --body '{"s3_bucket":"gorggle-dev-uploads","s3_video_key":"test.mp4","fps":25}' \
  --content-type application/json \
  --region us-east-1 \
  output.json
```

## Troubleshooting

### Common Issues

1. **Model not found on Hub**: Verify `SilentSpeak/LipCoordNet` exists on HuggingFace
2. **dlib not available**: Falls back to basic frame extraction without landmarks
3. **Video download fails**: Check S3 permissions and bucket/key
4. **Out of memory**: Use larger instance or reduce video length

### CloudWatch Logs
```
/aws/sagemaker/Endpoints/gorggle-lipcoordnet-dev
```

## Migration Complete! ðŸŽ‰

The system is now ready for deployment with a much simpler, more maintainable architecture.
