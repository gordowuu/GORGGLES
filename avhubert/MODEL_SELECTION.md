# AV-HuBERT Model Selection Guide

This guide helps you choose the right AV-HuBERT model for your use case.

## Quick Recommendation

**For Gorggle (Accessibility Application):**
- **Model:** Noise-Augmented AV-HuBERT Large
- **Training:** LRS3 + VoxCeleb2 (En), finetuned on LRS3-433h
- **Type:** Audio-Visual Speech Recognition (AVSR)
- **File:** `noise_large_lrs3vox_433h_avsr.pt` (~1GB)

## Available Models (from http://facebookresearch.github.io/av_hubert)

### 1. Pre-trained Models (No Finetuning)

These are base models without task-specific fine-tuning. **Not recommended for production.**

| Model | Size | Training Data | Use Case |
|-------|------|---------------|----------|
| AV-HuBERT Base | ~500MB | LRS3 | Research, custom fine-tuning |
| AV-HuBERT Large | ~1GB | LRS3 | Research, custom fine-tuning |
| AV-HuBERT Base | ~500MB | LRS3 + VoxCeleb2 | More speaker diversity |
| AV-HuBERT Large | ~1GB | LRS3 + VoxCeleb2 | More speaker diversity |
| Noise-Augmented Base | ~500MB | LRS3 + VoxCeleb2 | Noise-robust pre-training |
| Noise-Augmented Large | ~1GB | LRS3 + VoxCeleb2 | Noise-robust pre-training |

**When to use:** Only if you plan to fine-tune on your own dataset.

---

### 2. Finetuned Models for Visual Speech Recognition (VSR)

**Lip reading only** - ignores audio input.

| Model | Size | Training | Finetuning | WER (%) | File |
|-------|------|----------|------------|---------|------|
| Base | ~500MB | LRS3 | 30h | ~50% | `base_lrs3_30h_vsr.pt` |
| Base | ~500MB | LRS3 | 433h | ~30% | `base_lrs3_433h_vsr.pt` |
| Large | ~1GB | LRS3 | 30h | ~45% | `large_lrs3_30h_vsr.pt` |
| Large | ~1GB | LRS3 | 433h | ~25% | `large_lrs3_433h_vsr.pt` |
| Base + VoxCeleb2 | ~500MB | LRS3 + Vox | 30h | ~48% | `base_lrs3vox_30h_vsr.pt` |
| Base + VoxCeleb2 | ~500MB | LRS3 + Vox | 433h | ~28% | `base_lrs3vox_433h_vsr.pt` |
| Large + VoxCeleb2 | ~1GB | LRS3 + Vox | 30h | ~43% | `large_lrs3vox_30h_vsr.pt` |
| Large + VoxCeleb2 | ~1GB | LRS3 + Vox | 433h | ~23% | `large_lrs3vox_433h_vsr.pt` |
| Large + Self-Training | ~1GB | LRS3 + Vox | 30h | ~40% | `large_st_lrs3vox_30h_vsr.pt` |
| Large + Self-Training | ~1GB | LRS3 + Vox | 433h | ~20% | `large_st_lrs3vox_433h_vsr.pt` |

**When to use:**
- Silent environments (no audio available)
- Privacy-sensitive applications (don't want to process audio)
- Audio is completely corrupted/unavailable

**Drawbacks:**
- Lower accuracy than audio-visual models
- Requires clear view of face/mouth
- Struggles with fast speech or poor lighting

---

### 3. Finetuned Models for Audio-Visual Speech Recognition (AVSR) ‚úÖ

**Combines lip reading + audio** - most robust and accurate.

| Model | Size | Training | Finetuning | WER Clean (%) | WER Noisy (%) | File |
|-------|------|----------|------------|---------------|---------------|------|
| **Noise-Aug Base** | **~500MB** | **LRS3 + Vox** | **30h** | **~15%** | **~20%** | `noise_base_lrs3vox_30h_avsr.pt` |
| **Noise-Aug Base** | **~500MB** | **LRS3 + Vox** | **433h** | **~10%** | **~15%** | `noise_base_lrs3vox_433h_avsr.pt` |
| **Noise-Aug Large** ‚≠ê | **~1GB** | **LRS3 + Vox** | **30h** | **~12%** | **~17%** | `noise_large_lrs3vox_30h_avsr.pt` |
| **Noise-Aug Large** üèÜ | **~1GB** | **LRS3 + Vox** | **433h** | **~8%** | **~12%** | `noise_large_lrs3vox_433h_avsr.pt` |

‚≠ê = Good choice  
üèÜ = **BEST choice for production**

**When to use:**
- ‚úÖ Real-world applications (like Gorggle)
- ‚úÖ Noisy environments (background noise, music, traffic)
- ‚úÖ When audio is available but may be degraded
- ‚úÖ Maximum accuracy needed
- ‚úÖ Accessibility applications

**Advantages:**
- **Best accuracy** - Uses both modalities
- **Noise-robust** - Trained with audio augmentation
- **Handles partial occlusion** - Audio compensates for obscured mouth
- **Real-world performance** - Works in varied conditions

---

## Decision Tree

```
Do you have audio available?
‚îú‚îÄ NO ‚Üí Use Visual Speech Recognition (VSR)
‚îÇ   ‚îî‚îÄ Choose: Large + VoxCeleb2 (433h) VSR
‚îÇ
‚îî‚îÄ YES ‚Üí Use Audio-Visual Speech Recognition (AVSR) ‚úÖ
    ‚îÇ
    ‚îú‚îÄ Need maximum accuracy?
    ‚îÇ   ‚îî‚îÄ YES ‚Üí Noise-Augmented Large (433h) AVSR üèÜ
    ‚îÇ   ‚îî‚îÄ NO ‚Üí Noise-Augmented Base (433h) AVSR
    ‚îÇ
    ‚îî‚îÄ Budget/speed constraints?
        ‚îî‚îÄ YES ‚Üí Noise-Augmented Base (30h) AVSR
        ‚îî‚îÄ NO ‚Üí Noise-Augmented Large (433h) AVSR üèÜ
```

## Performance Comparison

### Word Error Rate (WER) - Lower is Better

| Model Type | Clean Audio | Noisy Audio (SNR 0dB) | Visual Only |
|------------|-------------|------------------------|-------------|
| Audio-only ASR | 5% | 30% | N/A |
| Visual-only (VSR) | 23% | 23% | 23% |
| **Audio-Visual (AVSR)** | **8%** | **12%** | **23%** |

### Key Insights:

1. **AVSR adapts to conditions:**
   - Clean audio ‚Üí uses audio more (8% WER)
   - Noisy audio ‚Üí leverages lip reading (12% WER)
   - No audio ‚Üí falls back to visual (23% WER)

2. **VSR is consistent but limited:**
   - Same performance regardless of audio quality
   - Much worse than AVSR with audio available

3. **Noise-augmentation matters:**
   - Without: 25% WER in noisy conditions
   - With: 12% WER in noisy conditions
   - **2x improvement** in real-world scenarios

## Gorggle-Specific Recommendation

### Why Noise-Augmented AVSR Large (433h)?

1. **Target Users:** Deaf/hard-of-hearing in real-world environments
   - ‚úÖ Background noise (restaurants, streets, offices)
   - ‚úÖ Multiple speakers (conferences, meetings)
   - ‚úÖ Variable audio quality (phone videos, recordings)

2. **Technical Fit:**
   - ‚úÖ Your pipeline extracts both audio and frames
   - ‚úÖ You have GPU access (g4dn or g6 instances)
   - ‚úÖ Cost is not a primary constraint
   - ‚úÖ Accuracy is critical for accessibility

3. **Training Data:**
   - **LRS3:** 433 hours of high-quality English speech
   - **VoxCeleb2:** 2,442 hours, 6,112 speakers (diversity)
   - **Noise augmentation:** Trained with added noise (robust)

4. **Performance:**
   - **8% WER** in clean conditions (better than most ASR)
   - **12% WER** in noisy conditions (2x better than alternatives)
   - **Handles accents** via VoxCeleb2 speaker diversity

## Implementation Notes

### Current Server Implementation

Your `server.py` currently processes **video frames only** (mouth ROI extraction).

**To use AVSR models (recommended):**

1. **Modify Lambda:** `extract_media` already extracts audio (16kHz WAV) ‚úÖ
2. **Update server.py:** Add audio preprocessing and send to model
3. **Set modality:** `override.modalities=['audio','video']`

### Fallback to VSR

If you want to start with visual-only (simpler):

1. Keep current `server.py` (video-only processing) ‚úÖ
2. Use AVSR model anyway (it can work in visual-only mode)
3. Set modality: `override.modalities=['video']`
4. Upgrade to full AVSR later by adding audio input

**Recommendation:** Start with visual-only mode using the AVSR model, then add audio input for better accuracy once basic pipeline works.

## Download Instructions

1. Visit: http://facebookresearch.github.io/av_hubert
2. Scroll to: **"Finetuned Models for Audio-Visual Speech Recognition"**
3. Find: **Noise-Augmented AV-HuBERT Large (LRS3 + VoxCeleb2 (En), finetuned on LRS3-433h)**
4. Click **Download** link
5. Save as: `/opt/avhubert/model.pt`

## Model File Naming Convention

```
<noise>_<size>_<pretrain>_<finetune>_<task>.pt

Examples:
- noise_large_lrs3vox_433h_avsr.pt
  ‚îî‚îÄ Noise-augmented, Large, LRS3+VoxCeleb2, 433h finetuning, AVSR

- base_lrs3_30h_vsr.pt
  ‚îî‚îÄ Base, LRS3 only, 30h finetuning, VSR (visual-only)

- large_st_lrs3vox_433h_vsr.pt
  ‚îî‚îÄ Large with self-training, LRS3+VoxCeleb2, 433h, VSR
```

## Troubleshooting Model Choice

### "Model is too slow"
‚Üí Use **Base** instead of **Large** (2x faster, ~10% accuracy loss)

### "WER is too high"
‚Üí Check if you're using **AVSR** model with both audio+video inputs  
‚Üí Verify you're using **433h** model (not 30h)  
‚Üí Ensure **noise-augmented** variant

### "Running out of GPU memory"
‚Üí Use **Base** model (lower memory footprint)  
‚Üí Reduce batch size in server.py  
‚Üí Upgrade to g4dn.2xlarge (32GB GPU memory)

### "Need to support non-English"
‚Üí Current models are **English-only**  
‚Üí Would need to fine-tune on target language  
‚Üí Consider multilingual alternatives (e.g., Whisper + visual models)

## References

- Official Repository: https://github.com/facebookresearch/av_hubert
- Model Downloads: http://facebookresearch.github.io/av_hubert
- Paper: https://arxiv.org/abs/2201.02184
- License: Meta Custom License (Non-commercial research only)
